#!/usr/bin/env python3
"""
G√©n√©rateur de m√©tadonn√©es intelligent pour les archives Pac-Man.

Fonctionnalit√©s :
- G√©n√©ration automatique de params.md avec explications contextuelles
- Analyse comparative avec sessions pr√©c√©dentes
- Observations automatiques bas√©es sur les m√©triques
- √âvaluation intelligente des hyperparam√®tres
- G√©n√©ration de tags et cat√©gories
"""

import json
import yaml
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
import statistics
from pathlib import Path

@dataclass
class TrainingMetrics:
    """M√©triques d'entra√Ænement pour une session."""
    total_episodes: int
    win_rate: float
    avg_score: float
    max_score: float
    min_score: float
    avg_steps: float
    exploration_rate: float
    learning_rate: float
    gamma: float
    epsilon: float
    batch_size: int
    buffer_size: int
    training_time_hours: float
    memory_usage_mb: float

@dataclass
class SessionMetadata:
    """M√©tadonn√©es compl√®tes d'une session."""
    session_id: str
    session_number: int
    timestamp: str
    model_type: str
    agent_type: str
    environment: str
    metrics: TrainingMetrics
    tags: List[str]
    previous_session_id: Optional[str] = None
    notes: Optional[str] = None
    config_hash: Optional[str] = None

class IntelligentMetadataGenerator:
    """
    G√©n√©rateur de m√©tadonn√©es intelligent.
    
    Produit des descriptions contextuelles, des comparaisons et des observations
    automatiques bas√©es sur les donn√©es d'entra√Ænement.
    """
    
    def __init__(self, history_file: Optional[str] = None):
        """
        Initialise le g√©n√©rateur avec un fichier d'historique optionnel.
        
        Args:
            history_file: Chemin vers un fichier JSON contenant l'historique des sessions
        """
        self.history_file = history_file
        self.session_history: List[Dict] = []
        
        if history_file and Path(history_file).exists():
            self._load_history()
    
    def _load_history(self) -> None:
        """Charge l'historique des sessions depuis le fichier."""
        try:
            with open(self.history_file, 'r') as f:
                self.session_history = json.load(f)
        except Exception as e:
            print(f"Warning: Could not load history file: {e}")
            self.session_history = []
    
    def _save_history(self) -> None:
        """Sauvegarde l'historique des sessions dans le fichier."""
        if not self.history_file:
            return
        
        try:
            with open(self.history_file, 'w') as f:
                json.dump(self.session_history, f, indent=2)
        except Exception as e:
            print(f"Warning: Could not save history file: {e}")
    
    def generate_params_md(self, current_session: SessionMetadata, 
                          previous_session: Optional[SessionMetadata] = None) -> str:
        """
        G√©n√®re le contenu du fichier params.md avec explications contextuelles.
        
        Args:
            current_session: M√©tadonn√©es de la session actuelle
            previous_session: M√©tadonn√©es de la session pr√©c√©dente (optionnel)
            
        Returns:
            Contenu Markdown format√©
        """
        lines = []
        
        # En-t√™te
        lines.append(f"# Session {current_session.session_number} - {current_session.timestamp}")
        lines.append("")
        lines.append(f"**Mod√®le**: {current_session.model_type}  ")
        lines.append(f"**Agent**: {current_session.agent_type}  ")
        lines.append(f"**Environnement**: {current_session.environment}")
        lines.append("")
        
        # Section 1: R√©sum√© ex√©cutif
        lines.append("## üìä R√©sum√© ex√©cutif")
        lines.append("")
        
        summary = self._generate_executive_summary(current_session.metrics, previous_session.metrics if previous_session else None)
        lines.append(summary)
        lines.append("")
        
        # Section 2: Param√®tres d'entra√Ænement avec √©valuation
        lines.append("## ‚öôÔ∏è Param√®tres d'entra√Ænement")
        lines.append("")
        
        params_evaluation = self._evaluate_hyperparameters(current_session.metrics)
        for param, (value, evaluation) in params_evaluation.items():
            lines.append(f"- **{param}**: `{value}` ‚Äì {evaluation}")
        lines.append("")
        
        # Section 3: M√©triques de performance d√©taill√©es
        lines.append("## üìà M√©triques de performance")
        lines.append("")
        
        metrics_table = self._format_metrics_table(current_session.metrics)
        lines.append(metrics_table)
        lines.append("")
        
        # Section 4: Analyse comparative (si session pr√©c√©dente disponible)
        if previous_session:
            lines.append("## üîÑ Comparaison avec session pr√©c√©dente")
            lines.append("")
            
            comparison = self._generate_comparison_analysis(current_session, previous_session)
            lines.append(comparison)
            lines.append("")
            
            # Graphique ASCII simple pour visualiser l'am√©lioration
            improvement_chart = self._generate_improvement_chart(
                current_session.metrics.win_rate,
                previous_session.metrics.win_rate
            )
            if improvement_chart:
                lines.append("### Tendance du taux de victoire")
                lines.append("```")
                lines.append(improvement_chart)
                lines.append("```")
                lines.append("")
        
        # Section 5: Observations et recommandations
        lines.append("## üí° Observations et recommandations")
        lines.append("")
        
        observations = self._generate_observations(current_session, previous_session)
        lines.append(observations)
        lines.append("")
        
        # Section 6: Tags et cat√©gories
        lines.append("## üè∑Ô∏è Tags et cat√©gories")
        lines.append("")
        
        tags_with_icons = self._categorize_session(current_session)
        lines.append(", ".join(tags_with_icons))
        lines.append("")
        
        # Section 7: Notes techniques
        lines.append("## üìù Notes techniques")
        lines.append("")
        
        if current_session.notes:
            lines.append(current_session.notes)
        else:
            lines.append("*Aucune note suppl√©mentaire.*")
        lines.append("")
        
        # Pied de page
        lines.append("---")
        lines.append(f"*G√©n√©r√© automatiquement le {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*")
        lines.append("*Syst√®me d'archivage intelligent - Laboratoire IA Pac-Man*")
        
        return "\n".join(lines)
    
    def _generate_executive_summary(self, metrics: TrainingMetrics, 
                                   previous_metrics: Optional[TrainingMetrics] = None) -> str:
        """G√©n√®re un r√©sum√© ex√©cutif des performances."""
        summary_parts = []
        
        # √âvaluation du taux de victoire
        if metrics.win_rate >= 0.8:
            winrate_eval = "excellente"
        elif metrics.win_rate >= 0.6:
            winrate_eval = "bonne"
        elif metrics.win_rate >= 0.4:
            winrate_eval = "moyenne"
        else:
            winrate_eval = "faible"
        
        summary_parts.append(f"Performance **{winrate_eval}** avec un taux de victoire de **{metrics.win_rate:.1%}**.")
        
        # Comparaison avec session pr√©c√©dente
        if previous_metrics:
            winrate_diff = metrics.win_rate - previous_metrics.win_rate
            if abs(winrate_diff) < 0.01:
                diff_text = "stable"
            elif winrate_diff > 0:
                diff_text = f"en am√©lioration de **+{winrate_diff:.1%}**"
            else:
                diff_text = f"en r√©gression de **{winrate_diff:.1%}**"
            
            summary_parts.append(f"Performance {diff_text} par rapport √† la session pr√©c√©dente.")
        
        # √âvaluation de l'efficacit√© d'entra√Ænement
        episodes_per_hour = metrics.total_episodes / max(metrics.training_time_hours, 0.1)
        if episodes_per_hour > 1000:
            efficiency = "tr√®s efficace"
        elif episodes_per_hour > 500:
            efficiency = "efficace"
        elif episodes_per_hour > 100:
            efficiency = "mod√©r√©ment efficace"
        else:
            efficiency = "peu efficace"
        
        summary_parts.append(f"Entra√Ænement **{efficiency}** ({episodes_per_hour:.0f} √©pisodes/heure).")
        
        # Recommandation globale
        if metrics.win_rate >= 0.7 and metrics.avg_score > 1000:
            recommendation = "La configuration actuelle est optimale. Poursuivre l'entra√Ænement."
        elif metrics.win_rate < 0.3:
            recommendation = "Revoir les hyperparam√®tres ou augmenter l'exploration."
        else:
            recommendation = "Continuer l'entra√Ænement avec ajustements mineurs si n√©cessaire."
        
        summary_parts.append(f"**Recommandation**: {recommendation}")
        
        return " ".join(summary_parts)
    
    def _evaluate_hyperparameters(self, metrics: TrainingMetrics) -> Dict[str, Tuple[Any, str]]:
        """√âvalue les hyperparam√®tres et retourne des descriptions contextuelles."""
        evaluations = {}
        
        # Learning Rate
        lr = metrics.learning_rate
        if lr > 0.01:
            lr_eval = "√©lev√© ‚Üí apprentissage rapide mais risque d'instabilit√©"
        elif lr > 0.001:
            lr_eval = "optimal ‚Üí bon compromis stabilit√©/vitesse"
        elif lr > 0.0001:
            lr_eval = "faible ‚Üí convergence lente mais stable"
        else:
            lr_eval = "tr√®s faible ‚Üí risque de sous-apprentissage"
        evaluations["Learning Rate"] = (lr, lr_eval)
        
        # Gamma (facteur de discount)
        gamma = metrics.gamma
        if gamma > 0.95:
            gamma_eval = "√©lev√© ‚Üí forte importance des r√©compenses futures"
        elif gamma > 0.85:
            gamma_eval = "mod√©r√© ‚Üí √©quilibre court/long terme"
        else:
            gamma_eval = "faible ‚Üí focus sur r√©compenses imm√©diates"
        evaluations["Gamma"] = (gamma, gamma_eval)
        
        # Epsilon (exploration)
        epsilon = metrics.epsilon
        if epsilon > 0.3:
            epsilon_eval = "√©lev√© ‚Üí forte exploration, bonne d√©couverte"
        elif epsilon > 0.1:
            epsilon_eval = "mod√©r√© ‚Üí bon √©quilibre exploration/exploitation"
        else:
            epsilon_eval = "faible ‚Üí forte exploitation, risque de stagnation"
        evaluations["Epsilon"] = (epsilon, epsilon_eval)
        
        # Batch Size
        batch = metrics.batch_size
        if batch > 128:
            batch_eval = "grand ‚Üí mise √† jour stable mais co√ªteuse"
        elif batch > 32:
            batch_eval = "moyen ‚Üí bon compromis"
        else:
            batch_eval = "petit ‚Üí mise √† jour fr√©quente mais bruyante"
        evaluations["Batch Size"] = (batch, batch_eval)
        
        return evaluations
    
    def _format_metrics_table(self, metrics: TrainingMetrics) -> str:
        """Formate les m√©triques dans un tableau Markdown."""
        table = [
            "| M√©trique | Valeur | √âvaluation |",
            "|----------|--------|------------|"
        ]
        
        # Taux de victoire
        winrate_eval = self._evaluate_metric(metrics.win_rate, "win_rate")
        table.append(f"| Taux de victoire | {metrics.win_rate:.2%} | {winrate_eval} |")
        
        # Score moyen
        score_eval = self._evaluate_metric(metrics.avg_score, "avg_score")
        table.append(f"| Score moyen | {metrics.avg_score:.0f} | {score_eval} |")
        
        # √âpisodes
        table.append(f"| √âpisodes totaux | {metrics.total_episodes} | - |")
        
        # Temps d'entra√Ænement
        table.append(f"| Dur√©e d'entra√Ænement | {metrics.training_time_hours:.1f}h | - |")
        
        # Utilisation m√©moire
        mem_eval = self._evaluate_metric(metrics.memory_usage_mb, "memory")
        table.append(f"| M√©moire utilis√©e | {metrics.memory_usage_mb:.0f} MB | {mem_eval} |")
        
        return "\n".join(table)
    
    def _evaluate_metric(self, value: float, metric_type: str) -> str:
        """√âvalue une m√©trique sp√©cifique."""
        if metric_type == "win_rate":
            if value >= 0.8:
                return "‚≠ê Excellente"
            elif value >= 0.6:
                return "‚úÖ Bonne"
            elif value >= 0.4:
                return "‚ö†Ô∏è Moyenne"
            else:
                return "‚ùå √Ä am√©liorer"
        
        elif metric_type == "avg_score":
            if value >= 2000:
                return "‚≠ê Exceptionnel"
            elif value >= 1000:
                return "‚úÖ Bon"
            elif value >= 500:
                return "‚ö†Ô∏è Acceptable"
            else:
                return "‚ùå Faible"
        
        elif metric_type == "memory":
            if value > 2000:
                return "‚ö†Ô∏è √âlev√©e"
            elif value > 1000:
                return "‚úÖ Normale"
            else:
                return "‚úÖ Optimale"
        
        return "-"
    
    def _generate_comparison_analysis(self, current: SessionMetadata, 
                                     previous: SessionMetadata) -> str:
        """G√©n√®re une analyse comparative entre deux sessions."""
        analysis = []
        
        # Diff√©rences de m√©triques
        winrate_diff = current.metrics.win_rate - previous.metrics.win_rate
        score_diff = current.metrics.avg_score - previous.metrics.avg_score
        episodes_diff = current.metrics.total_episodes - previous.metrics.total_episodes
        
        # Analyse du taux de victoire
        if abs(winrate_diff) < 0.01:
            winrate_analysis = "**Stabilit√©** du taux de victoire."
        elif winrate_diff > 0.15:
            winrate_analysis = f"**Am√©lioration significative** (+{winrate_diff:.1%}) !"
        elif winrate_diff > 0.05:
            winrate_analysis = f"**Am√©lioration mod√©r√©e** (+{winrate_diff:.1%})."
        elif winrate_diff > -0.05:
            winrate_analysis = "**L√©g√®re variation** dans la marge d'erreur."
        elif winrate_diff > -0.15:
            winrate_analysis = f"**L√©g√®re r√©gression** ({winrate_diff:.1%})."
        else:
            winrate_analysis = f"**R√©gression significative** ({winrate_diff:.1%}) !"
        
        analysis.append(f"- **Taux de victoire**: {winrate_analysis}")
        
        # Analyse du score
        if abs(score_diff) < 50:
            score_analysis = "Score stable."
        elif score_diff > 200:
            score_analysis = f"Score **fortement am√©lior√©** (+{score_diff:.0f})."
        elif score_diff > 0:
            score_analysis = f"Score **l√©g√®rement am√©lior√©** (+{score_diff:.0f})."
        else:
            score_analysis = f"Score **en baisse** ({score_diff:.0f})."
        
        analysis.append(f"- **Score moyen**: {score_analysis}")
        
        # Analyse des √©pisodes
        if episodes_diff > 0:
            episodes_analysis = f"**+{episodes_diff} √©pisodes** d'entra√Ænement suppl√©mentaires."
        else:
            episodes_analysis = f"**{episodes_diff} √©pisodes** de moins."
        
        analysis.append(f"- **Volume d'entra√Ænement**: {episodes_analysis}")
        
        # Conclusion comparative
        if winrate_diff > 0.1 and score_diff > 100:
            conclusion = "**Progression nette** dans toutes les m√©triques. La configuration actuelle est sup√©rieure."
        elif winrate_diff > 0 and score_diff > 0:
            conclusion = "**Progression positive**. L'entra√Ænement porte ses fruits."
        elif abs(winrate_diff) < 0.05 and abs(score_diff) < 100:
            conclusion = "**Stabilit√© g√©n√©rale**. Possible plateau d'apprentissage."
        else:
            conclusion = "**Performance en baisse**. Revoir la strat√©gie d'entra√Ænement."
        
        analysis.append(f"\n**Conclusion**: {conclusion}")
        
        return "\n".join(analysis)
    def _generate_improvement_chart(self, current_winrate: float,
                                   previous_winrate: float) -> str:
        """G√©n√®re un graphique ASCII simple pour visualiser l'am√©lioration."""
        if previous_winrate <= 0:
            return ""
        
        # Normaliser les valeurs pour un graphique de 20 caract√®res
        max_value = max(current_winrate, previous_winrate, 0.01)
        scale = 20 / max_value
        
        prev_bars = int(previous_winrate * scale)
        curr_bars = int(current_winrate * scale)
        
        # Cr√©er les barres
        prev_bar = "‚ñà" * prev_bars + "‚ñë" * (20 - prev_bars)
        curr_bar = "‚ñà" * curr_bars + "‚ñë" * (20 - curr_bars)
        
        chart = [
            f"Pr√©c√©dent [{previous_winrate:.1%}]: {prev_bar}",
            f"Actuel    [{current_winrate:.1%}]: {curr_bar}",
            "",
            f"√âvolution: {'‚Üë' if current_winrate > previous_winrate else '‚Üì'} {abs(current_winrate - previous_winrate):.1%}"
        ]
        
        return "\n".join(chart)
    
    def _generate_observations(self, current: SessionMetadata,
                              previous: Optional[SessionMetadata] = None) -> str:
        """G√©n√®re des observations et recommandations automatiques."""
        observations = []
        
        # Observation bas√©e sur le taux de victoire
        if current.metrics.win_rate >= 0.8:
            observations.append("‚úÖ **Performance excellente** ‚Äì Le mod√®le ma√Ætrise bien l'environnement.")
        elif current.metrics.win_rate >= 0.6:
            observations.append("‚úÖ **Performance satisfaisante** ‚Äì Bon √©quilibre exploration/exploitation.")
        elif current.metrics.win_rate >= 0.4:
            observations.append("‚ö†Ô∏è **Performance moyenne** ‚Äì Possibilit√© d'am√©lioration avec ajustement des hyperparam√®tres.")
        else:
            observations.append("‚ùå **Performance faible** ‚Äì Revoir la strat√©gie d'entra√Ænement.")
        
        # Observation bas√©e sur la stabilit√© du score
        score_range = current.metrics.max_score - current.metrics.min_score
        if score_range > 2000:
            observations.append("‚ö†Ô∏è **Grande variabilit√© des scores** ‚Äì L'entra√Ænement est instable. Essayer de r√©duire le learning rate.")
        elif score_range < 500:
            observations.append("‚úÖ **Scores stables** ‚Äì L'entra√Ænement converge bien.")
        
        # Observation bas√©e sur l'exploration
        if current.metrics.exploration_rate > 0.3:
            observations.append("üîç **Exploration √©lev√©e** ‚Äì Le mod√®le explore activement. Bon pour d√©couvrir de nouvelles strat√©gies.")
        elif current.metrics.exploration_rate < 0.05:
            observations.append("üéØ **Exploitation √©lev√©e** ‚Äì Le mod√®le exploite ses connaissances. Risque de stagnation.")
        
        # Recommandations bas√©es sur la comparaison
        if previous:
            winrate_diff = current.metrics.win_rate - previous.metrics.win_rate
            
            if winrate_diff > 0.1:
                observations.append("üöÄ **Progression rapide** ‚Äì Maintenir la configuration actuelle.")
            elif winrate_diff < -0.1:
                observations.append("üîß **R√©gression d√©tect√©e** ‚Äì Revenir aux hyperparam√®tres pr√©c√©dents ou augmenter l'exploration.")
            elif abs(winrate_diff) < 0.02:
                observations.append("‚è∏Ô∏è **Plateau d√©tect√©** ‚Äì Essayer de nouvelles strat√©gies d'exploration ou ajuster le learning rate.")
        
        # Recommandation finale
        if current.metrics.win_rate < 0.3:
            observations.append("\n**üéØ Recommandation prioritaire**: Augmenter le taux d'exploration (epsilon) et r√©duire le learning rate.")
        elif current.metrics.win_rate > 0.7:
            observations.append("\n**üéØ Recommandation**: Poursuivre l'entra√Ænement pour consolider les performances.")
        else:
            observations.append("\n**üéØ Recommandation**: Ajuster progressivement les hyperparam√®tres pour am√©liorer les performances.")
        
        return "\n\n".join(observations)
    
    def _categorize_session(self, session: SessionMetadata) -> List[str]:
        """Cat√©gorise la session et g√©n√®re des tags avec ic√¥nes."""
        tags = []
        
        # Bas√© sur le taux de victoire
        if session.metrics.win_rate >= 0.8:
            tags.append("üèÜ excellence")
        elif session.metrics.win_rate >= 0.6:
            tags.append("‚úÖ bonne_performance")
        elif session.metrics.win_rate >= 0.4:
            tags.append("‚ö†Ô∏è performance_moyenne")
        else:
            tags.append("üîß besoin_am√©lioration")
        
        # Bas√© sur le type de mod√®le
        if "dqn" in session.model_type.lower():
            tags.append("üß† DQN")
        elif "ppo" in session.model_type.lower():
            tags.append("üîÑ PPO")
        elif "a2c" in session.model_type.lower():
            tags.append("‚ö° A2C")
        else:
            tags.append(f"ü§ñ {session.model_type}")
        
        # Bas√© sur l'agent
        if "pacman" in session.agent_type.lower():
            tags.append("üëª PacMan")
        elif "ghost" in session.agent_type.lower():
            tags.append("üëª Fant√¥me")
        else:
            tags.append(f"üéÆ {session.agent_type}")
        
        # Bas√© sur la dur√©e d'entra√Ænement
        if session.metrics.training_time_hours > 10:
            tags.append("‚è≥ long_entra√Ænement")
        elif session.metrics.training_time_hours > 1:
            tags.append("‚è±Ô∏è entra√Ænement_moyen")
        else:
            tags.append("‚ö° entra√Ænement_court")
        
        # Tags personnalis√©s de la session
        tags.extend(session.tags)
        
        return tags
    
    def generate_config_yaml(self, session: SessionMetadata) -> str:
        """G√©n√®re un fichier de configuration YAML pour la session."""
        config = {
            'session': {
                'id': session.session_id,
                'number': session.session_number,
                'timestamp': session.timestamp,
                'model_type': session.model_type,
                'agent_type': session.agent_type,
                'environment': session.environment
            },
            'hyperparameters': {
                'learning_rate': session.metrics.learning_rate,
                'gamma': session.metrics.gamma,
                'epsilon': session.metrics.epsilon,
                'batch_size': session.metrics.batch_size,
                'buffer_size': session.metrics.buffer_size
            },
            'performance': {
                'win_rate': session.metrics.win_rate,
                'avg_score': session.metrics.avg_score,
                'total_episodes': session.metrics.total_episodes,
                'training_time_hours': session.metrics.training_time_hours
            },
            'tags': session.tags,
            'notes': session.notes or ""
        }
        
        return yaml.dump(config, default_flow_style=False, allow_unicode=True)
    
    def generate_metadata_json(self, session: SessionMetadata) -> Dict[str, Any]:
        """G√©n√®re un dictionnaire de m√©tadonn√©es au format JSON."""
        return {
            'session': asdict(session),
            'generated_at': datetime.now().isoformat(),
            'generator_version': '1.0.0',
            'analysis': {
                'performance_category': self._categorize_session(session)[0].replace("Ô∏è", "").strip(),
                'recommendations': self._generate_observations(session, None).split("\n\n")[:3]
            }
        }


# Exemple d'utilisation
if __name__ == "__main__":
    print("=== Test du g√©n√©rateur de m√©tadonn√©es intelligent ===")
    
    # Cr√©er des m√©triques de test
    test_metrics = TrainingMetrics(
        total_episodes=5000,
        win_rate=0.77,
        avg_score=1520.5,
        max_score=2450,
        min_score=620,
        avg_steps=850,
        exploration_rate=0.15,
        learning_rate=0.001,
        gamma=0.99,
        epsilon=0.1,
        batch_size=32,
        buffer_size=10000,
        training_time_hours=2.5,
        memory_usage_mb=1240
    )
    
    # Cr√©er une session de test
    test_session = SessionMetadata(
        session_id="test_session_001",
        session_number=47,
        timestamp="2026-01-03T16:32:00",
        model_type="DQN",
        agent_type="PacMan",
        environment="PacMan-v0",
        metrics=test_metrics,
        tags=['baseline', 'DQN', 'test_run'],
        notes="Session de test pour validation du g√©n√©rateur de m√©tadonn√©es."
    )
    
    # Cr√©er une session pr√©c√©dente pour comparaison
    prev_metrics = TrainingMetrics(
        total_episodes=3000,
        win_rate=0.65,
        avg_score=1380,
        max_score=2100,
        min_score=580,
        avg_steps=790,
        exploration_rate=0.2,
        learning_rate=0.0015,
        gamma=0.95,
        epsilon=0.15,
        batch_size=32,
        buffer_size=10000,
        training_time_hours=1.8,
        memory_usage_mb=1180
    )
    
    prev_session = SessionMetadata(
        session_id="test_session_046",
        session_number=46,
        timestamp="2026-01-02T14:20:00",
        model_type="DQN",
        agent_type="PacMan",
        environment="PacMan-v0",
        metrics=prev_metrics,
        tags=['baseline', 'DQN']
    )
    
    # Initialiser le g√©n√©rateur
    generator = IntelligentMetadataGenerator()
    
    # G√©n√©rer le params.md
    params_content = generator.generate_params_md(test_session, prev_session)
    
    print("\n=== Contenu g√©n√©r√© (extrait) ===")
    print(params_content[:500] + "...")
    
    # G√©n√©rer la configuration YAML
    yaml_content = generator.generate_config_yaml(test_session)
    print("\n=== Configuration YAML ===")
    print(yaml_content)
    
    # G√©n√©rer les m√©tadonn√©es JSON
    json_metadata = generator.generate_metadata_json(test_session)
    print("\n=== M√©tadonn√©es JSON (extrait) ===")
    print(json.dumps(json_metadata, indent=2)[:500] + "...")
    
    print("\n=== Test termin√© avec succ√®s ===")
        
